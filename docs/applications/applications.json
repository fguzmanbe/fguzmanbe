[
  {
    "path": "applications/2023-11-14-linear-regression/",
    "title": "An Introduction to Statistical Learning",
    "description": "Embarking on an exploration into the realm of data science, I'm immersing myself in 'An Introduction to Statistical Learning with Applications in R' by James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). Focused on chapters 3 and 4, I'll replicate exercises within this enlightening book to uncover the secrets concealed within datasets.",
    "author": [
      {
        "name": "Francisco Guzmán",
        "url": {}
      }
    ],
    "date": "2023-11-14",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n3.6 Lab: Linear Regression\r\n3.6.1 Libraries\r\n3.6.2 Simple Linear Regression\r\n3.6.3 Multiple Linear Regression\r\n3.6.4 Interaction Terms\r\n3.6.5 Non-linear Transformations of the Predictors\r\n3.6.6 Qualitative Predictors\r\n3.6.7 Writing Functions\r\n\r\n4.6 Lab\r\n4.6.1 The Stock Market Data\r\n4.6.2 Logistic Regression\r\n\r\n\r\n3.6 Lab: Linear Regression\r\n3.6.1 Libraries\r\n\r\n\r\nlibrary(MASS)\r\nlibrary(ISLR)\r\n\r\n\r\n3.6.2 Simple Linear Regression\r\n\r\n\r\nnames(Boston)\r\n\r\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"     \r\n [7] \"age\"     \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"  \r\n[13] \"lstat\"   \"medv\"   \r\n\r\nWe will start by using the lm() function to fit a simple linear regression model, with \\(medv\\) as the response and \\(lstat\\) as the predictor. The basic syntax is lm(y~x,data), where \\(y\\) is the response, \\(x\\) is the predictor, and data is the data set in which these two variables are kept.\r\n\r\n\r\nlm.fit=lm(medv~lstat, data=Boston)\r\n\r\n\r\nFor more detailed information, we use summary(lm.fit).\r\n\r\n\r\nsummary(lm.fit)\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ lstat, data = Boston)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-15.168  -3.990  -1.318   2.034  24.500 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\r\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 6.216 on 504 degrees of freedom\r\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \r\nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\r\n\r\ncoef(lm.fit)\r\n\r\n(Intercept)       lstat \r\n 34.5538409  -0.9500494 \r\n\r\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\r\n\r\n\r\nconfint(lm.fit)\r\n\r\n                2.5 %     97.5 %\r\n(Intercept) 33.448457 35.6592247\r\nlstat       -1.026148 -0.8739505\r\n\r\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of \\(medv\\) for a given value of \\(lstat\\).\r\n\r\n\r\npredict(lm.fit, data.frame(lstat=c(5,10 ,15)), interval = \"confidence\")\r\n\r\n       fit      lwr      upr\r\n1 29.80359 29.00741 30.59978\r\n2 25.05335 24.47413 25.63256\r\n3 20.30310 19.73159 20.87461\r\n\r\npredict(lm.fit ,data.frame(lstat=c(5,10 ,15)), interval = \"prediction\")\r\n\r\n       fit       lwr      upr\r\n1 29.80359 17.565675 42.04151\r\n2 25.05335 12.827626 37.27907\r\n3 20.30310  8.077742 32.52846\r\n\r\nWe will now plot \\(medv\\) and \\(lstat\\) along with the least squares regression\r\nline using the plot() and abline() functions.\r\n\r\n\r\nattach(Boston)\r\nplot(lstat, medv)\r\nabline(lm.fit)\r\n\r\n\r\n\r\nBelow we experiment with some additional settings for plotting lines and points. The lwd=3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\r\n\r\n\r\nplot(lstat, medv)\r\nabline (lm.fit, lwd =3)\r\nabline (lm.fit, lwd=3,col =\"red\")\r\n\r\n\r\nplot(lstat, medv, col=\"red\")\r\n\r\n\r\nplot(lstat, medv, pch =20)\r\n\r\n\r\nplot(lstat, medv, pch =\"+\")\r\n\r\n\r\nplot(1:25,1:25, pch =1:25)\r\n\r\n\r\n\r\nIn general, plot() command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() function, which tells R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow=c(2,2)) divides the plotting region into a 2 × 2 grid of panels.\r\n\r\n\r\npar(mfrow=c(2,2))\r\nplot(lm.fit)\r\n\r\n\r\n\r\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\r\n\r\n\r\nplot(predict (lm.fit), residuals (lm.fit))\r\n\r\n\r\nplot(predict (lm.fit), rstudent (lm.fit))\r\n\r\n\r\n\r\nOn the basis of the residual plots, there is some evidence of non-linearity.\r\nLeverage statistics can be computed for any number of predictors using the hatvalues() function.\r\n\r\n\r\nplot(hatvalues (lm.fit))\r\n\r\n\r\nwhich.max(hatvalues (lm.fit))\r\n\r\n375 \r\n375 \r\n\r\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage\r\nstatistic.\r\n3.6.3 Multiple Linear Regression\r\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y~x1+x2+x3) is used to fit a model with three predictors, \\(x1\\), \\(x2\\), and \\(x3\\). The summary() function now outputs the regression coefficients for all the predictors.\r\n\r\n\r\nlm.fit=lm(medv~lstat+age, data=Boston)\r\nsummary (lm.fit)\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ lstat + age, data = Boston)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-15.981  -3.978  -1.283   1.968  23.158 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\r\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\r\nage          0.03454    0.01223   2.826  0.00491 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 6.173 on 503 degrees of freedom\r\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \r\nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\r\n\r\nThe Boston data set contains 13 variables, and so it would be cumbersome\r\nto have to type all of these in order to perform a regression using all of the\r\npredictors. Instead, we can use the following short-hand:\r\n\r\n\r\nlm.fit=lm(medv~.,data=Boston)\r\nsummary (lm.fit)\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ ., data = Boston)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-15.595  -2.730  -0.518   1.777  26.199 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\r\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \r\nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\r\nindus        2.056e-02  6.150e-02   0.334 0.738288    \r\nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \r\nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\r\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\r\nage          6.922e-04  1.321e-02   0.052 0.958229    \r\ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\r\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\r\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \r\nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\r\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\r\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 4.745 on 492 degrees of freedom\r\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \r\nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\r\n\r\nThe vif() function, part of the car package, can be used to compute variance inflation factors.\r\n\r\n\r\nvif(lm.fit)\r\n\r\n    crim       zn    indus     chas      nox       rm      age \r\n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 \r\n     dis      rad      tax  ptratio    black    lstat \r\n3.955945 7.484496 9.008554 1.799084 1.348521 2.941491 \r\n\r\nThe following syntax results in a regression using all predictors except age.\r\n\r\n\r\nlm.fit1=lm(medv~.-age, data=Boston)\r\nsummary(lm.fit1)\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ . - age, data = Boston)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\r\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \r\nzn            0.046334   0.013613   3.404 0.000719 ***\r\nindus         0.020562   0.061433   0.335 0.737989    \r\nchas          2.689026   0.859598   3.128 0.001863 ** \r\nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\r\nrm            3.814394   0.408480   9.338  < 2e-16 ***\r\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\r\nrad           0.305786   0.066089   4.627 4.75e-06 ***\r\ntax          -0.012329   0.003755  -3.283 0.001099 ** \r\nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\r\nblack         0.009321   0.002678   3.481 0.000544 ***\r\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 4.74 on 493 degrees of freedom\r\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \r\nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\r\n\r\nAlternatively, the update() function can be used.\r\n\r\n\r\nlm.fit1=update(lm.fit, ~.-age)\r\n\r\n\r\n3.6.4 Interaction Terms\r\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells R to include an interaction term between \\(lstat\\) and \\(black\\). The syntax lstat\\(*\\)age simultaneously includes \\(lstat\\), \\(age\\), and the interaction term \\(lstat×age\\) as predictors; it is a shorthand for\r\nlstat+age+lstat:age.\r\n\r\n\r\nsummary (lm(medv~lstat*age, data=Boston))\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ lstat * age, data = Boston)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-15.806  -4.045  -1.333   2.085  27.552 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\r\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\r\nage         -0.0007209  0.0198792  -0.036   0.9711    \r\nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 6.149 on 502 degrees of freedom\r\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \r\nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\r\n\r\n3.6.5 Non-linear Transformations of the Predictors\r\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise \\(X\\) to the power \\(2\\). We now perform a regression of \\(medv\\) onto \\(lstat\\) and \\(lstat^2\\).\r\n\r\n\r\nlm.fit2 = lm(medv ~ lstat + I(lstat^2))\r\nsummary(lm.fit2)\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ lstat + I(lstat^2))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\r\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\r\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 5.524 on 503 degrees of freedom\r\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \r\nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\r\n\r\nThe near-zero p-value associated with the quadratic term suggests that\r\nit leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\r\n\r\n\r\nlm.fit = lm(medv~lstat)\r\nanova(lm.fit, lm.fit2)\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: medv ~ lstat\r\nModel 2: medv ~ lstat + I(lstat^2)\r\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \r\n1    504 19472                                 \r\n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nHere Model 1 represents the linear submodel containing only one predictor, \\(lstat\\), while Model 2 corresponds to the larger quadratic model that has two predictors, \\(lstat\\) and \\(lstat^2\\). The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors \\(lstat\\) and \\(lstat^2\\) is far superior to the model that only contains the predictor \\(lstat\\). This is not surprising, since earlier we saw evidence for non-linearity in the relationship between \\(medv\\) and \\(lstat\\). If we type\r\n\r\n\r\npar(mfrow=c(2,2))\r\nplot(lm.fit2)\r\n\r\n\r\n\r\nthen we see that when the \\(lstat^2\\) term is included in the model, there is\r\nlittle discernible pattern in the residuals.\r\nIn order to create a cubic fit, we can include a predictor of the form\r\nI(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\r\n\r\n\r\nlm.fit5=lm(medv ~ poly(lstat ,5))\r\nsummary (lm.fit5)\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ poly(lstat, 5))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\r\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\r\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\r\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\r\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\r\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 5.215 on 500 degrees of freedom\r\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \r\nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\r\n\r\nThis suggests that including additional polynomial terms, up to fifth order,\r\nleads to an improvement in the model fit! However, further investigation of\r\nthe data reveals that no polynomial terms beyond fifth order have significant p-values in a regression fit.\r\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation\r\n\r\n\r\n summary(lm(medv~log(rm),data=Boston))\r\n\r\n\r\nCall:\r\nlm(formula = medv ~ log(rm), data = Boston)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-19.487  -2.875  -0.104   2.837  39.816 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\r\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 6.915 on 504 degrees of freedom\r\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \r\nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\r\n\r\n3.6.6 Qualitative Predictors\r\nWe will now examine the Carseats data, which is part of the ISLR library. We will attempt to predict \\(Sales\\) (child car seat sales) in 400 locations based on a number of predictors.\r\n\r\n\r\nnames(Carseats)\r\n\r\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\"\r\n [5] \"Population\"  \"Price\"       \"ShelveLoc\"   \"Age\"        \r\n [9] \"Education\"   \"Urban\"       \"US\"         \r\n\r\nThe Carseats data includes qualitative predictors such as \\(Shelveloc\\), an indicator of the quality of the shelving location —that is, the space within\r\na store in which the car seat is displayed— at each location. The predictor \\(Shelveloc\\) takes on three possible values, Bad, Medium, and Good.\r\nGiven a qualitative variable such as \\(Shelveloc\\), R generates dummy variables\r\nautomatically. Below we fit a multiple regression model that includes some\r\ninteraction terms.\r\n\r\n\r\nlm.fit=lm(Sales~.+Income:Advertising + Price:Age,data=Carseats)\r\nsummary (lm.fit)\r\n\r\n\r\nCall:\r\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.9208 -0.7503  0.0177  0.6754  3.3413 \r\n\r\nCoefficients:\r\n                     Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\r\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\r\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\r\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \r\nPopulation          0.0001592  0.0003679   0.433 0.665330    \r\nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\r\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\r\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\r\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\r\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \r\nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \r\nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \r\nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \r\nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.011 on 386 degrees of freedom\r\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \r\nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\r\n\r\nThe contrasts() function returns the coding that R uses for the dummy variables.\r\n\r\n\r\nattach(Carseats)\r\ncontrasts(ShelveLoc)\r\n\r\n       Good Medium\r\nBad       0      0\r\nGood      1      0\r\nMedium    0      1\r\n\r\nR has created a \\(ShelveLocGood\\) dummy variable that takes on a value of\r\n1 if the shelving location is good, and 0 otherwise. It has also created a\r\n\\(ShelveLocMedium\\) dummy variable that equals 1 if the shelving location is\r\nmedium, and 0 otherwise. A bad shelving location corresponds to a zero\r\nfor each of the two dummy variables. The fact that the coefficient for \\(ShelveLocGood\\) in the regression output is positive indicates that a good\r\nshelving location is associated with high sales (relative to a bad location).\r\nAnd \\(ShelveLocMedium\\) has a smaller positive coefficient, indicating that a\r\nmedium shelving location leads to higher sales than a bad shelving location\r\nbut lower sales than a good shelving location.\r\n3.6.7 Writing Functions\r\nAs we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR and MASS libraries, called LoadLibraries().\r\n\r\n\r\nLoadLibraries = function(){\r\n  library(ISLR)\r\n  library(MASS)\r\n  print(\"The libraries have been loaded.\")\r\n }\r\n\r\n\r\nNow if we type in LoadLibraries, R will tell us what is in the function.\r\n\r\n\r\nLoadLibraries\r\n\r\nfunction(){\r\n  library(ISLR)\r\n  library(MASS)\r\n  print(\"The libraries have been loaded.\")\r\n }\r\n\r\nIf we call the function, the libraries are loaded in and the print statement\r\nis output.\r\n\r\n\r\nLoadLibraries()\r\n\r\n[1] \"The libraries have been loaded.\"\r\n\r\n4.6 Lab\r\n4.6.1 The Stock Market Data\r\n\r\n\r\nlibrary(ISLR)\r\nstr(Smarket)\r\n\r\n'data.frame':   1250 obs. of  9 variables:\r\n $ Year     : num  2001 2001 2001 2001 2001 ...\r\n $ Lag1     : num  0.381 0.959 1.032 -0.623 0.614 ...\r\n $ Lag2     : num  -0.192 0.381 0.959 1.032 -0.623 ...\r\n $ Lag3     : num  -2.624 -0.192 0.381 0.959 1.032 ...\r\n $ Lag4     : num  -1.055 -2.624 -0.192 0.381 0.959 ...\r\n $ Lag5     : num  5.01 -1.055 -2.624 -0.192 0.381 ...\r\n $ Volume   : num  1.19 1.3 1.41 1.28 1.21 ...\r\n $ Today    : num  0.959 1.032 -0.623 0.614 0.213 ...\r\n $ Direction: Factor w/ 2 levels \"Down\",\"Up\": 2 2 1 2 2 2 1 2 2 2 ...\r\n\r\nsummary(Smarket)\r\n\r\n      Year           Lag1                Lag2          \r\n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000  \r\n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500  \r\n Median :2003   Median : 0.039000   Median : 0.039000  \r\n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919  \r\n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \r\n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000  \r\n      Lag3                Lag4                Lag5         \r\n Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.92200  \r\n 1st Qu.:-0.640000   1st Qu.:-0.640000   1st Qu.:-0.64000  \r\n Median : 0.038500   Median : 0.038500   Median : 0.03850  \r\n Mean   : 0.001716   Mean   : 0.001636   Mean   : 0.00561  \r\n 3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.59700  \r\n Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.73300  \r\n     Volume           Today           Direction \r\n Min.   :0.3561   Min.   :-4.922000   Down:602  \r\n 1st Qu.:1.2574   1st Qu.:-0.639500   Up  :648  \r\n Median :1.4229   Median : 0.038500             \r\n Mean   :1.4783   Mean   : 0.003138             \r\n 3rd Qu.:1.6417   3rd Qu.: 0.596750             \r\n Max.   :3.1525   Max.   : 5.733000             \r\n\r\ncor(Smarket [,-9])\r\n\r\n             Year         Lag1         Lag2         Lag3         Lag4\r\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\r\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\r\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\r\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\r\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\r\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\r\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\r\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\r\n               Lag5      Volume        Today\r\nYear    0.029787995  0.53900647  0.030095229\r\nLag1   -0.005674606  0.04090991 -0.026155045\r\nLag2   -0.003557949 -0.04338321 -0.010250033\r\nLag3   -0.018808338 -0.04182369 -0.002447647\r\nLag4   -0.027083641 -0.04841425 -0.006899527\r\nLag5    1.000000000 -0.02200231 -0.034860083\r\nVolume -0.022002315  1.00000000  0.014591823\r\nToday  -0.034860083  0.01459182  1.000000000\r\n\r\n\r\n\r\nattach(Smarket)\r\nplot(Volume)\r\n\r\n\r\n\r\n4.6.2 Logistic Regression\r\n\r\n\r\nglm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\r\ndata = Smarket, family = binomial)\r\nsummary(glm.fit)\r\n\r\n\r\nCall:\r\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \r\n    Volume, family = binomial, data = Smarket)\r\n\r\nDeviance Residuals: \r\n   Min      1Q  Median      3Q     Max  \r\n-1.446  -1.203   1.065   1.145   1.326  \r\n\r\nCoefficients:\r\n             Estimate Std. Error z value Pr(>|z|)\r\n(Intercept) -0.126000   0.240736  -0.523    0.601\r\nLag1        -0.073074   0.050167  -1.457    0.145\r\nLag2        -0.042301   0.050086  -0.845    0.398\r\nLag3         0.011085   0.049939   0.222    0.824\r\nLag4         0.009359   0.049974   0.187    0.851\r\nLag5         0.010313   0.049511   0.208    0.835\r\nVolume       0.135441   0.158360   0.855    0.392\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 1731.2  on 1249  degrees of freedom\r\nResidual deviance: 1727.6  on 1243  degrees of freedom\r\nAIC: 1741.6\r\n\r\nNumber of Fisher Scoring iterations: 3\r\n\r\ncoef(glm.fit)\r\n\r\n (Intercept)         Lag1         Lag2         Lag3         Lag4 \r\n-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938 \r\n        Lag5       Volume \r\n 0.010313068  0.135440659 \r\n\r\nsummary(glm.fit)$coef\r\n\r\n                Estimate Std. Error    z value  Pr(>|z|)\r\n(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\r\nLag1        -0.073073746 0.05016739 -1.4565986 0.1452272\r\nLag2        -0.042301344 0.05008605 -0.8445733 0.3983491\r\nLag3         0.011085108 0.04993854  0.2219750 0.8243333\r\nLag4         0.009358938 0.04997413  0.1872757 0.8514445\r\nLag5         0.010313068 0.04951146  0.2082966 0.8349974\r\nVolume       0.135440659 0.15835970  0.8552723 0.3924004\r\n\r\nsummary(glm.fit)$coef[,4] # Para obtener sólo los p-values\r\n\r\n(Intercept)        Lag1        Lag2        Lag3        Lag4 \r\n  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445 \r\n       Lag5      Volume \r\n  0.8349974   0.3924004 \r\n\r\nProbabilidad de que el mercado vaya al alza.\r\n\r\n\r\nglm.probs = predict(glm.fit, type = \"response\")\r\nglm.probs[1:10]\r\n\r\n        1         2         3         4         5         6         7 \r\n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 \r\n        8         9        10 \r\n0.5092292 0.5176135 0.4888378 \r\n\r\ncontrasts(Direction)\r\n\r\n     Up\r\nDown  0\r\nUp    1\r\n\r\n\r\n\r\nglm.pred = rep(\"Down\", 1250)\r\nglm.pred[glm.probs >.5] = \"Up\"\r\ntable(glm.pred, Direction)\r\n\r\n        Direction\r\nglm.pred Down  Up\r\n    Down  145 141\r\n    Up    457 507\r\n\r\nLos elementos de las diagonales indican predicciones correctas, el resto son predicciones incorrectas.\r\n\r\n\r\nmean(glm.pred==Direction) #Predicciones correctas\r\n\r\n[1] 0.5216\r\n\r\nCreamos un vector para las obs desde 2001 a 2004, a continuación, utilizaremos este vector para crear un conjunto de datos para las observaciones de 2005.\r\n\r\n\r\ntrain = (Year < 2005)\r\nSmarket.2005 = Smarket[!train,]\r\ndim(Smarket.2005)\r\n\r\n[1] 252   9\r\n\r\nDirection.2005 = Direction[!train]\r\n\r\n\r\n\r\n\r\nglm.fit= glm (Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\r\n              data = Smarket, family = binomial, subset = train)\r\nglm.probs = predict(glm.fit, Smarket.2005, type = \"response\")\r\n\r\n\r\n\r\n\r\nglm.pred = rep(\"Down\", 252)\r\nglm.pred[glm.probs >.5]= \"Up\"\r\ntable(glm.pred, Direction.2005)\r\n\r\n        Direction.2005\r\nglm.pred Down Up\r\n    Down   77 97\r\n    Up     34 44\r\n\r\nmean(glm.pred == Direction.2005)\r\n\r\n[1] 0.4801587\r\n\r\nmean(glm.pred != Direction.2005)\r\n\r\n[1] 0.5198413\r\n\r\nAjustamos la regresión logística utilizando sólo \\(Lag1\\) y \\(Lag2\\), que parecían tener el mayor poder de predicción en el modelo de regresión logística original.\r\n\r\n\r\nglm.fit = glm(Direction ~ Lag1 + Lag2, data = Smarket, family = binomial,\r\n            subset=train)\r\nglm.probs = predict(glm.fit, Smarket.2005, type= \"response\")\r\nglm.pred = rep(\"Down\", 252)\r\nglm.pred[glm.probs >.5] = \"Up\"\r\ntable(glm.pred, Direction.2005)\r\n\r\n        Direction.2005\r\nglm.pred Down  Up\r\n    Down   35  35\r\n    Up     76 106\r\n\r\nmean(glm.pred==Direction.2005)\r\n\r\n[1] 0.5595238\r\n\r\n\r\n\r\npredict(glm.fit, newdata = data.frame(Lag1=c(1.2 ,1.5),\r\n                                      Lag2=c(1.1,-0.8)),\r\n        type=\"response\")\r\n\r\n        1         2 \r\n0.4791462 0.4960939 \r\n\r\n\r\n\r\n\r\n",
    "preview": "applications/2023-11-14-linear-regression/distill-preview.png",
    "last_modified": "2023-11-15T22:27:59+00:00",
    "input_file": {}
  }
]
